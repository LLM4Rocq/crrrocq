
import json
import logging
import math
import re
from pathlib import Path
from typing import List, Mapping, Optional

import datasets
import numpy as np
import torch
from datasets import load_dataset

from nemo.collections.common.tokenizers import TokenizerSpec
from nemo.collections.llm.gpt.data.utils import (
    _JSONLMemMapDataset,
)
from nemo.core.classes import Dataset

logger = logging.getLogger(__name__)

# hack to avoid the "not enough disk space" error in some slurm cluster
datasets.builder.has_sufficient_disk_space = lambda needed_bytes, directory='.': True

PREFIX_STR = (
    "\x00"  # the prefix string used in the tokenizer to deal with the added empty token for some of the tokenizers
)

__idx_version__ = "0.2"  # index file version
__idx_suffix__ = "idx"  # index file suffix



class GPTSFTDatasetInterleaved(Dataset):
    """ """

    def __init__(
        self,
        file_path: str,
        prompt_path: str,
        tokenizer: TokenizerSpec,
        max_seq_length: int = 1024,
        min_seq_length: int = 1,
        pad_seq_length_to_mult: int = 16,
        seed: int = 1234,
        index_mapping_dir: str = None,
        tokens_to_generate: int = 0,
        memmap_workers: Optional[int] = None,
        is_test: bool = False
    ):
        """
        file_path: Path to a JSONL GPT supervised fine-tuning dataset.
            Data is formatted as multiple JSON lines with each line formatted as follows:
            {
                'input': 'John von Neumann\nVon Neumann made fundamental contributions ...
                    Q: What did the math of artificial viscosity do?',
                'output': 'smoothed the shock transition without sacrificing basic physics'
            }
        tokenizer: Tokenizer for the dataset. Instance of a class that inherits TokenizerSpec (ex: SentencePiece).
        max_seq_length (int): maximum sequence length for each dataset examples.
            Examples will either be truncated to fit this length or dropped if they cannot be truncated.
        min_seq_length (int): min length of each data example in the dataset.
            Data examples will be dropped if they do not meet the min length requirements.
        tokens_to_generate (int): (inference only) Number of tokens to generate during inference
        seed: Random seed for data shuffling.
        index_mapping_dir: Directory to save the index mapping to.
            If None, will write to the same folder as the dataset.
        is_test: Whether this dataset is the test split.
        """
        self.tokenizer = tokenizer
        self.file_path = file_path
        self.prompt_path = prompt_path
        self.max_seq_length = max_seq_length
        self.min_seq_length = min_seq_length
        self.pad_seq_length_to_mult = pad_seq_length_to_mult

        self.seed = seed

        self.index_mapping_dir = index_mapping_dir
        self.tokens_to_generate = tokens_to_generate
        self.memmap_workers = memmap_workers
        self.is_test = is_test

        self._load_dataset()
        self._load_prompt()

    def _load_prompt(self):
        with open(self.prompt_path, 'r') as file:
            self.prompt = json.load(file)

    def _load_dataset(self):
        self.indexed_dataset = _JSONLMemMapDataset(
            dataset_paths=[self.file_path],
            tokenizer=None,
            header_lines=0,
            index_mapping_dir=self.index_mapping_dir,
            workers=self.memmap_workers,
        )

    def __len__(self):
        return len(self.indexed_dataset)

    def __getitem__(self, idx):
        if isinstance(idx, np.int64):
            idx = idx.item()

        assert idx < len(self.indexed_dataset)
        # idx may < 0 because we pad_samples_to_global_batch_size, e.g. id = -1
        if idx < 0:
            idx = len(self) + idx
            auto_gen_idx = True
        else:
            auto_gen_idx = False
        try:
            example = self.indexed_dataset[idx]
            if auto_gen_idx:
                example['__AUTOGENERATED__'] = True
        except Exception as e:
            logger.error(f"Error while loading example {idx} from dataset {self.file_path}")
            raise e
        return self._process_example(example)

    def _truncation(self, ids, expect_length):
        return ids[:expect_length]

    def _process_example(self, example):
        """
        Create an example by concatenating reasoning block
        Truncation is carried out when needed.
        BOS, and EOS are added.
        """

        input_ids = self.tokenizer.text_to_ids(self.prompt['instruction'].format(initial_goal=example['initial_goal']))
        ignore_idx = len(input_ids) * [0]
    
        for block in example['blocks']:
            tag_beg_ids = self.tokenizer.text_to_ids(f"<{block['kind']}>\n")
            content_ids = self.tokenizer.text_to_ids(f"{block['content']}\n")
            tag_end_ids = self.tokenizer.text_to_ids(f"/<{block['kind']}>\n")
            input_ids += tag_beg_ids + content_ids + tag_end_ids
            ignore_idx += (len(tag_beg_ids) + len(content_ids) + len(tag_end_ids)) * [0 if block['ignore'] else 1]
        input_ids = input_ids + [self.tokenizer.eos_id]
        ignore_idx.append(1)
        processed_example = {
            'input_ids': input_ids,
            'ignore_idx': ignore_idx,
            'token_count': len(input_ids)
        }
        return processed_example

    def _maybe_cast_to_list(self, x):
        if isinstance(x, np.ndarray):
            return [item.tolist() for item in x]
        return x

    def _ceil_to_nearest(self, n, m):
        return (n + m - 1) // m * m

    def _collate_item(self, item, max_length, pad_id):
        item = self._maybe_cast_to_list(item)
        # max_length = max([len(x) for x in item]) if item else 0
        # here [0] should be tokenizer.pad_id
        item = [x + [pad_id] * (max_length - len(x)) for x in item]
        return item

    def collate_fn(self, batch):
        input_ids = [item['input_ids'][:-1] for item in batch]
        labels = [item['input_ids'][1:] for item in batch]
        loss_mask = [item['ignore_idx'][1:] for item in batch]
        token_count = [item['token_count'] for item in batch]

        max_length = max([len(x) for x in input_ids]) + self.tokens_to_generate
        # increase max length to nearest multiple of 4 or 8
        max_length = min(self.max_seq_length, self._ceil_to_nearest(max_length, self.pad_seq_length_to_mult))

        position_ids = [list(range(max_length)) for _ in batch]
        position_ids = torch.LongTensor(position_ids)
        input_ids = torch.LongTensor(
            self._collate_item(input_ids, max_length=max_length, pad_id=self.tokenizer.eos_id)
        )
        labels = torch.LongTensor(self._collate_item(labels, max_length=max_length, pad_id=self.tokenizer.eos_id))
        loss_mask = torch.LongTensor(self._collate_item(loss_mask, max_length=max_length, pad_id=0))

        processed_batch = {
            'tokens': input_ids,
            'labels': labels,
            'loss_mask': loss_mask,
            'position_ids': position_ids,
            'token_count': token_count,
        }

        return processed_batch